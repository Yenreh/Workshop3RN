{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "USE_CPU = False  # Cambiar a False para intentar usar GPU\n",
    "\n",
    "if USE_CPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    print(\" Usando CPU\")\n",
    "    \n",
    "else:\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "    print(\" Usando GPU\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reducir mensajes de TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c82f5c",
   "metadata": {},
   "source": [
    "# Taller 3 - Predicción de Series Temporales con LSTM y GRU\n",
    "## Análisis Completo de Experimentos\n",
    "\n",
    "Objetivos:\n",
    "1. Implementar modelos LSTM y GRU para predicción de precios de acciones\n",
    "2. Entrenar modelos con diferentes configuraciones de hiperparámetros\n",
    "3. Comparar resultados entre LSTM y GRU\n",
    "4. Evaluar modelos apilados (stacked) vs simples\n",
    "5. Analizar métricas: MSE, MAE, RMSE, R², MAPE\n",
    "\n",
    "Estructura del notebook:\n",
    "- Parte 1: Configuración e importaciones\n",
    "- Parte 2: Carga y exploración de datos\n",
    "- Parte 3: Experimentos con LSTM (6 configuraciones)\n",
    "- Parte 4: Experimentos con GRU (6 configuraciones)\n",
    "- Parte 5: Experimentos con modelos apilados (4 configuraciones)\n",
    "- Parte 6: Análisis comparativo y respuestas del taller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa3700",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 1: Configuración e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Importar módulos del proyecto\n",
    "from data_loader import load_and_prepare_data\n",
    "from preprocessing import prepare_sequences_for_training\n",
    "from models import create_lstm_model, create_gru_model, create_stacked_lstm_model, create_stacked_gru_model\n",
    "from utils import create_callbacks, evaluate_model, save_model_info, compare_models\n",
    "from visualize import (\n",
    "    plot_average_price, plot_data_splits, plot_training_history,\n",
    "    plot_predictions, plot_residuals, plot_model_comparison\n",
    ")\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Configurar directorio de salida\n",
    "OUTPUT_DIR = 'output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Módulos importados correctamente\")\n",
    "#print(f\"TensorFlow/Keras version: {keras.__version__}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8a6fe",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: Carga y Exploración de Datos\n",
    "\n",
    "### 2.1 Selección del Archivo de Datos\n",
    "\n",
    "Elegir un archivo de la carpeta `dataset/Data/Stocks/` o `dataset/Data/ETFs/`.\n",
    "Por defecto usaremos Apple (aapl.us.txt) como ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar archivo de stock\n",
    "STOCK_FILE = 'dataset/Data/Stocks/aapl.us.txt'\n",
    "\n",
    "# Verificar que el archivo existe\n",
    "if os.path.exists(STOCK_FILE):\n",
    "    print(f\"Archivo seleccionado: {STOCK_FILE}\")\n",
    "    stock_name = os.path.basename(STOCK_FILE).replace('.us.txt', '').upper()\n",
    "    print(f\"Stock: {stock_name}\")\n",
    "else:\n",
    "    print(f\"ERROR: El archivo {STOCK_FILE} no existe\")\n",
    "    print(\"\\nArchivos disponibles en dataset/Data/Stocks/:\")\n",
    "    stocks_dir = 'dataset/Data/Stocks/'\n",
    "    if os.path.exists(stocks_dir):\n",
    "        files = [f for f in os.listdir(stocks_dir) if f.endswith('.txt')][:10]\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")\n",
    "        print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b349f",
   "metadata": {},
   "source": [
    "### 2.2 Cargar y Procesar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808338e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos con splits 70/15/15\n",
    "print(\"Cargando datos...\")\n",
    "data_dict = load_and_prepare_data(\n",
    "    file_path=STOCK_FILE,\n",
    "    train_ratio=0.70,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15\n",
    ")\n",
    "\n",
    "# Extraer componentes\n",
    "df = data_dict['df']\n",
    "average_prices = data_dict['average_prices']\n",
    "train_data = data_dict['train_data']\n",
    "val_data = data_dict['val_data']\n",
    "test_data = data_dict['test_data']\n",
    "train_idx = data_dict['train_idx']\n",
    "val_idx = data_dict['val_idx']\n",
    "test_idx = data_dict['test_idx']\n",
    "\n",
    "print(f\"\\nTotal de registros: {len(df)}\")\n",
    "print(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n",
    "print(f\"\\nPrimeras fechas: {df['Date'].iloc[0]} a {df['Date'].iloc[4]}\")\n",
    "print(f\"Últimas fechas: {df['Date'].iloc[-5]} a {df['Date'].iloc[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974bfa9e",
   "metadata": {},
   "source": [
    "### 2.3 Visualizar Serie Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e67839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar precio promedio completo\n",
    "plot_average_price(df, stock_name)\n",
    "\n",
    "# Graficar splits de datos\n",
    "indices_dict = {\n",
    "    'train': train_idx,\n",
    "    'val': val_idx,\n",
    "    'test': test_idx\n",
    "}\n",
    "plot_data_splits(\n",
    "    average_prices,\n",
    "    indices_dict,\n",
    "    title=f\"División de Datos - {stock_name}\"\n",
    ")\n",
    "\n",
    "print(\"\\nEstadísticas del precio promedio:\")\n",
    "print(f\"  Min: ${average_prices.min():.2f}\")\n",
    "print(f\"  Max: ${average_prices.max():.2f}\")\n",
    "print(f\"  Mean: ${average_prices.mean():.2f}\")\n",
    "print(f\"  Std: ${average_prices.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26148563",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 3: Experimentos con LSTM\n",
    "\n",
    "### 3.1 Definición de Experimentos LSTM\n",
    "\n",
    "Se definen 6 configuraciones para evaluar diferentes hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array de experimentos para LSTM\n",
    "lstm_experiments = [\n",
    "    {\n",
    "        'name': 'LSTM Baseline',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 16,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'LSTM Seq=120',\n",
    "        'seq_length': 120,\n",
    "        'batch_size': 16,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'LSTM Seq=180',\n",
    "        'seq_length': 180,\n",
    "        'batch_size': 16,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'LSTM Batch=32',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 32,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'LSTM Batch=64',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 64,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'LSTM Dropout=0.2',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 16,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.2,\n",
    "        'epochs': 50\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Definidos {len(lstm_experiments)} experimentos para LSTM\")\n",
    "for i, exp in enumerate(lstm_experiments, 1):\n",
    "    print(f\"  {i}. {exp['name']}: seq={exp['seq_length']}, batch={exp['batch_size']}, dropout={exp['recurrent_dropout']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e517f5d",
   "metadata": {},
   "source": [
    "### 3.2 Ejecución de Experimentos LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar todos los experimentos de LSTM\n",
    "lstm_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INICIANDO EXPERIMENTOS CON LSTM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, config in enumerate(lstm_experiments, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENTO LSTM {i}/{len(lstm_experiments)}: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Preparar secuencias con la longitud específica\n",
    "    print(f\"Preparando secuencias (seq_length={config['seq_length']})...\")\n",
    "    sequences = prepare_sequences_for_training(\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        seq_length=config['seq_length']\n",
    "    )\n",
    "    \n",
    "    X_train = sequences['X_train']\n",
    "    y_train = sequences['y_train']\n",
    "    X_val = sequences['X_val']\n",
    "    y_val = sequences['y_val']\n",
    "    X_test = sequences['X_test']\n",
    "    y_test = sequences['y_test']\n",
    "    normalizer = sequences['normalizer']\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # Crear modelo LSTM\n",
    "    print(\"Creando modelo LSTM...\")\n",
    "    model = create_lstm_model(\n",
    "        input_shape=(config['seq_length'], 1),\n",
    "        units=config['units'],\n",
    "        recurrent_dropout=config['recurrent_dropout']\n",
    "    )\n",
    "    \n",
    "    # Crear callbacks\n",
    "    model_name = config['name'].replace(' ', '_').lower()\n",
    "    callbacks = create_callbacks(\n",
    "        model_name=model_name,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        patience=10\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(f\"Entrenando con batch_size={config['batch_size']}, epochs={config['epochs']}...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    print(\"\\nEvaluando modelo...\")\n",
    "    metrics = evaluate_model(model, X_test, y_test, normalizer)\n",
    "    \n",
    "    print(f\"\\nMétricas en Test Set:\")\n",
    "    print(f\"  MSE:  {metrics['mse']:.4f}\")\n",
    "    print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"  R²:   {metrics['r2']:.4f}\")\n",
    "    print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar información del modelo\n",
    "    save_model_info(model, config, metrics, model_name, OUTPUT_DIR)\n",
    "    \n",
    "    # Visualizar historial de entrenamiento\n",
    "    plot_training_history(\n",
    "        history, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_history.png\"\n",
    "    )\n",
    "    \n",
    "    # Hacer predicciones y visualizar\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_original = normalizer.inverse_transform(y_pred)\n",
    "    y_test_original = normalizer.inverse_transform(y_test)\n",
    "    \n",
    "    plot_predictions(\n",
    "        y_test_original, \n",
    "        y_pred_original, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_predictions.png\"\n",
    "    )\n",
    "    \n",
    "    plot_residuals(\n",
    "        y_test_original, \n",
    "        y_pred_original, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_residuals.png\"\n",
    "    )\n",
    "    \n",
    "    # Guardar resultados\n",
    "    lstm_results.append({\n",
    "        'config': config,\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'metrics': metrics,\n",
    "        'normalizer': normalizer\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"COMPLETADOS {len(lstm_results)} EXPERIMENTOS CON LSTM\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b0f9b",
   "metadata": {},
   "source": [
    "### 3.3 Comparación de Resultados LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895dcae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame comparativo\n",
    "comparison_data = []\n",
    "for result in lstm_results:\n",
    "    comparison_data.append({\n",
    "        'Modelo': result['config']['name'],\n",
    "        'MSE': result['metrics']['mse'],\n",
    "        'MAE': result['metrics']['mae'],\n",
    "        'RMSE': result['metrics']['rmse'],\n",
    "        'R²': result['metrics']['r2'],\n",
    "        'MAPE (%)': result['metrics']['mape']\n",
    "    })\n",
    "\n",
    "lstm_comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACIÓN DE RESULTADOS - LSTM\")\n",
    "print(\"=\"*80)\n",
    "print(lstm_comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualizar comparación\n",
    "plot_model_comparison(\n",
    "    lstm_comparison_df,\n",
    "    metric='mape',\n",
    "    title=f\"Comparación de Modelos LSTM - {stock_name}\",\n",
    "    save_path=f\"{OUTPUT_DIR}/lstm_comparison.png\"\n",
    ")\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_idx = lstm_comparison_df['MAPE (%)'].idxmin()\n",
    "best_model = lstm_comparison_df.iloc[best_idx]\n",
    "print(f\"\\nMejor modelo LSTM (menor MAPE): {best_model['Modelo']}\")\n",
    "print(f\"  MAPE: {best_model['MAPE (%)']:.2f}%\")\n",
    "print(f\"  R²: {best_model['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727daf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 4: Experimentos con GRU\n",
    "\n",
    "### 4.1 Definición de Experimentos GRU\n",
    "\n",
    "Se definen 6 configuraciones equivalentes a LSTM para comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array de experimentos para GRU (mismas configuraciones que LSTM)\n",
    "gru_experiments = [\n",
    "    {\n",
    "        'name': 'GRU Baseline',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 16,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'GRU Seq=120',\n",
    "        'seq_length': 120,\n",
    "        'batch_size': 16,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'GRU Seq=180',\n",
    "        'seq_length': 180,\n",
    "        'batch_size': 16,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'GRU Batch=32',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 32,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'GRU Batch=64',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 64,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.0,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'GRU Dropout=0.2',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 16,\n",
    "        'units': 64,\n",
    "        'recurrent_dropout': 0.2,\n",
    "        'epochs': 50\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Definidos {len(gru_experiments)} experimentos para GRU\")\n",
    "for i, exp in enumerate(gru_experiments, 1):\n",
    "    print(f\"  {i}. {exp['name']}: seq={exp['seq_length']}, batch={exp['batch_size']}, dropout={exp['recurrent_dropout']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0f9b5",
   "metadata": {},
   "source": [
    "### 4.2 Ejecución de Experimentos GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ce2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar todos los experimentos de GRU\n",
    "gru_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INICIANDO EXPERIMENTOS CON GRU\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, config in enumerate(gru_experiments, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENTO GRU {i}/{len(gru_experiments)}: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Preparar secuencias con la longitud específica\n",
    "    print(f\"Preparando secuencias (seq_length={config['seq_length']})...\")\n",
    "    sequences = prepare_sequences_for_training(\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        seq_length=config['seq_length']\n",
    "    )\n",
    "    \n",
    "    X_train = sequences['X_train']\n",
    "    y_train = sequences['y_train']\n",
    "    X_val = sequences['X_val']\n",
    "    y_val = sequences['y_val']\n",
    "    X_test = sequences['X_test']\n",
    "    y_test = sequences['y_test']\n",
    "    normalizer = sequences['normalizer']\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # Crear modelo GRU\n",
    "    print(\"Creando modelo GRU...\")\n",
    "    model = create_gru_model(\n",
    "        input_shape=(config['seq_length'], 1),\n",
    "        units=config['units'],\n",
    "        recurrent_dropout=config['recurrent_dropout']\n",
    "    )\n",
    "    \n",
    "    # Crear callbacks\n",
    "    model_name = config['name'].replace(' ', '_').lower()\n",
    "    callbacks = create_callbacks(\n",
    "        model_name=model_name,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        patience=10\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(f\"Entrenando con batch_size={config['batch_size']}, epochs={config['epochs']}...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    print(\"\\nEvaluando modelo...\")\n",
    "    metrics = evaluate_model(model, X_test, y_test, normalizer)\n",
    "    \n",
    "    print(f\"\\nMétricas en Test Set:\")\n",
    "    print(f\"  MSE:  {metrics['mse']:.4f}\")\n",
    "    print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"  R²:   {metrics['r2']:.4f}\")\n",
    "    print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar información del modelo\n",
    "    save_model_info(model, config, metrics, model_name, OUTPUT_DIR)\n",
    "    \n",
    "    # Visualizar historial de entrenamiento\n",
    "    plot_training_history(\n",
    "        history, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_history.png\"\n",
    "    )\n",
    "    \n",
    "    # Hacer predicciones y visualizar\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_original = normalizer.inverse_transform(y_pred)\n",
    "    y_test_original = normalizer.inverse_transform(y_test)\n",
    "    \n",
    "    plot_predictions(\n",
    "        y_test_original, \n",
    "        y_pred_original, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_predictions.png\"\n",
    "    )\n",
    "    \n",
    "    plot_residuals(\n",
    "        y_test_original, \n",
    "        y_pred_original, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_residuals.png\"\n",
    "    )\n",
    "    \n",
    "    # Guardar resultados\n",
    "    gru_results.append({\n",
    "        'config': config,\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'metrics': metrics,\n",
    "        'normalizer': normalizer\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"COMPLETADOS {len(gru_results)} EXPERIMENTOS CON GRU\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dafb65",
   "metadata": {},
   "source": [
    "### 4.3 Comparación de Resultados GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11000e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame comparativo\n",
    "comparison_data = []\n",
    "for result in gru_results:\n",
    "    comparison_data.append({\n",
    "        'Modelo': result['config']['name'],\n",
    "        'MSE': result['metrics']['mse'],\n",
    "        'MAE': result['metrics']['mae'],\n",
    "        'RMSE': result['metrics']['rmse'],\n",
    "        'R²': result['metrics']['r2'],\n",
    "        'MAPE (%)': result['metrics']['mape']\n",
    "    })\n",
    "\n",
    "gru_comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACIÓN DE RESULTADOS - GRU\")\n",
    "print(\"=\"*80)\n",
    "print(gru_comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualizar comparación\n",
    "plot_model_comparison(\n",
    "    gru_comparison_df,\n",
    "    metric='mape',\n",
    "    title=f\"Comparación de Modelos GRU - {stock_name}\",\n",
    "    save_path=f\"{OUTPUT_DIR}/gru_comparison.png\"\n",
    ")\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_idx = gru_comparison_df['MAPE (%)'].idxmin()\n",
    "best_model = gru_comparison_df.iloc[best_idx]\n",
    "print(f\"\\nMejor modelo GRU (menor MAPE): {best_model['Modelo']}\")\n",
    "print(f\"  MAPE: {best_model['MAPE (%)']:.2f}%\")\n",
    "print(f\"  R²: {best_model['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859af06",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 5: Experimentos con Modelos Apilados (Stacked)\n",
    "\n",
    "### 5.1 Definición de Experimentos Stacked\n",
    "\n",
    "Se definen 4 configuraciones para evaluar modelos apilados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array de experimentos para modelos apilados\n",
    "stacked_experiments = [\n",
    "    {\n",
    "        'name': 'Stacked LSTM 2-layers',\n",
    "        'model_type': 'lstm',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 16,\n",
    "        'units_list': [64, 32],\n",
    "        'recurrent_dropout': 0.2,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'Stacked LSTM 3-layers',\n",
    "        'model_type': 'lstm',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 16,\n",
    "        'units_list': [64, 32, 16],\n",
    "        'recurrent_dropout': 0.2,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'Stacked GRU 2-layers',\n",
    "        'model_type': 'gru',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 16,\n",
    "        'units_list': [64, 32],\n",
    "        'recurrent_dropout': 0.2,\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'name': 'Stacked GRU 3-layers',\n",
    "        'model_type': 'gru',\n",
    "        'seq_length': 60,\n",
    "        'batch_size': 16,\n",
    "        'units_list': [64, 32, 16],\n",
    "        'recurrent_dropout': 0.2,\n",
    "        'epochs': 50\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Definidos {len(stacked_experiments)} experimentos para modelos apilados\")\n",
    "for i, exp in enumerate(stacked_experiments, 1):\n",
    "    layers_str = '-'.join(map(str, exp['units_list']))\n",
    "    print(f\"  {i}. {exp['name']}: {exp['model_type'].upper()} [{layers_str}], dropout={exp['recurrent_dropout']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358271a4",
   "metadata": {},
   "source": [
    "### 5.2 Ejecución de Experimentos Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar todos los experimentos stacked\n",
    "stacked_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INICIANDO EXPERIMENTOS CON MODELOS APILADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, config in enumerate(stacked_experiments, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENTO STACKED {i}/{len(stacked_experiments)}: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Preparar secuencias\n",
    "    print(f\"Preparando secuencias (seq_length={config['seq_length']})...\")\n",
    "    sequences = prepare_sequences_for_training(\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        seq_length=config['seq_length']\n",
    "    )\n",
    "    \n",
    "    X_train = sequences['X_train']\n",
    "    y_train = sequences['y_train']\n",
    "    X_val = sequences['X_val']\n",
    "    y_val = sequences['y_val']\n",
    "    X_test = sequences['X_test']\n",
    "    y_test = sequences['y_test']\n",
    "    normalizer = sequences['normalizer']\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # Crear modelo stacked\n",
    "    print(f\"Creando modelo {config['model_type'].upper()} apilado con capas {config['units_list']}...\")\n",
    "    if config['model_type'] == 'lstm':\n",
    "        model = create_stacked_lstm_model(\n",
    "            input_shape=(config['seq_length'], 1),\n",
    "            units_list=config['units_list'],\n",
    "            recurrent_dropout=config['recurrent_dropout']\n",
    "        )\n",
    "    else:  # gru\n",
    "        model = create_stacked_gru_model(\n",
    "            input_shape=(config['seq_length'], 1),\n",
    "            units_list=config['units_list'],\n",
    "            recurrent_dropout=config['recurrent_dropout']\n",
    "        )\n",
    "    \n",
    "    # Crear callbacks\n",
    "    model_name = config['name'].replace(' ', '_').lower()\n",
    "    callbacks = create_callbacks(\n",
    "        model_name=model_name,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        patience=10\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(f\"Entrenando con batch_size={config['batch_size']}, epochs={config['epochs']}...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    print(\"\\nEvaluando modelo...\")\n",
    "    metrics = evaluate_model(model, X_test, y_test, normalizer)\n",
    "    \n",
    "    print(f\"\\nMétricas en Test Set:\")\n",
    "    print(f\"  MSE:  {metrics['mse']:.4f}\")\n",
    "    print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"  R²:   {metrics['r2']:.4f}\")\n",
    "    print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
    "    \n",
    "    # Guardar información del modelo\n",
    "    save_model_info(model, config, metrics, model_name, OUTPUT_DIR)\n",
    "    \n",
    "    # Visualizar historial de entrenamiento\n",
    "    plot_training_history(\n",
    "        history, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_history.png\"\n",
    "    )\n",
    "    \n",
    "    # Hacer predicciones y visualizar\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_original = normalizer.inverse_transform(y_pred)\n",
    "    y_test_original = normalizer.inverse_transform(y_test)\n",
    "    \n",
    "    plot_predictions(\n",
    "        y_test_original, \n",
    "        y_pred_original, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_predictions.png\"\n",
    "    )\n",
    "    \n",
    "    plot_residuals(\n",
    "        y_test_original, \n",
    "        y_pred_original, \n",
    "        config['name'],\n",
    "        save_path=f\"{OUTPUT_DIR}/{model_name}_residuals.png\"\n",
    "    )\n",
    "    \n",
    "    # Guardar resultados\n",
    "    stacked_results.append({\n",
    "        'config': config,\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'metrics': metrics,\n",
    "        'normalizer': normalizer\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"COMPLETADOS {len(stacked_results)} EXPERIMENTOS CON MODELOS APILADOS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d63aa4",
   "metadata": {},
   "source": [
    "### 5.3 Comparación de Resultados Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame comparativo\n",
    "comparison_data = []\n",
    "for result in stacked_results:\n",
    "    comparison_data.append({\n",
    "        'Modelo': result['config']['name'],\n",
    "        'MSE': result['metrics']['mse'],\n",
    "        'MAE': result['metrics']['mae'],\n",
    "        'RMSE': result['metrics']['rmse'],\n",
    "        'R²': result['metrics']['r2'],\n",
    "        'MAPE (%)': result['metrics']['mape']\n",
    "    })\n",
    "\n",
    "stacked_comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACIÓN DE RESULTADOS - MODELOS APILADOS\")\n",
    "print(\"=\"*80)\n",
    "print(stacked_comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualizar comparación\n",
    "plot_model_comparison(\n",
    "    stacked_comparison_df,\n",
    "    metric='mape',\n",
    "    title=f\"Comparación de Modelos Apilados - {stock_name}\",\n",
    "    save_path=f\"{OUTPUT_DIR}/stacked_comparison.png\"\n",
    ")\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_idx = stacked_comparison_df['MAPE (%)'].idxmin()\n",
    "best_model = stacked_comparison_df.iloc[best_idx]\n",
    "print(f\"\\nMejor modelo Stacked (menor MAPE): {best_model['Modelo']}\")\n",
    "print(f\"  MAPE: {best_model['MAPE (%)']:.2f}%\")\n",
    "print(f\"  R²: {best_model['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305918c0",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 6: Análisis Comparativo y Respuestas al Taller\n",
    "\n",
    "### 6.1 Comparación Global de Todos los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9816a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los resultados en un solo DataFrame\n",
    "all_comparison_df = pd.concat([\n",
    "    lstm_comparison_df,\n",
    "    gru_comparison_df,\n",
    "    stacked_comparison_df\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARACIÓN GLOBAL DE TODOS LOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "print(all_comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualizar comparación global\n",
    "plot_model_comparison(\n",
    "    all_comparison_df,\n",
    "    metric='mape',\n",
    "    title=f\"Comparación Global de Todos los Modelos - {stock_name}\",\n",
    "    save_path=f\"{OUTPUT_DIR}/global_comparison.png\"\n",
    ")\n",
    "\n",
    "# Identificar mejores modelos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 5 MODELOS (según MAPE)\")\n",
    "print(\"=\"*80)\n",
    "top_5 = all_comparison_df.nsmallest(5, 'MAPE (%)')\n",
    "for idx, row in top_5.iterrows():\n",
    "    print(f\"{idx+1}. {row['Modelo']}\")\n",
    "    print(f\"   MAPE: {row['MAPE (%)']:.2f}% | R²: {row['R²']:.4f} | RMSE: {row['RMSE']:.4f}\")\n",
    "\n",
    "# Mejor modelo overall\n",
    "best_idx = all_comparison_df['MAPE (%)'].idxmin()\n",
    "best_overall = all_comparison_df.iloc[best_idx]\n",
    "print(f\"\\n MEJOR MODELO GLOBAL: {best_overall['Modelo']}\")\n",
    "print(f\"   MAPE: {best_overall['MAPE (%)']:.2f}%\")\n",
    "print(f\"   R²: {best_overall['R²']:.4f}\")\n",
    "print(f\"   RMSE: {best_overall['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef850b0",
   "metadata": {},
   "source": [
    "### 6.2 Análisis Comparativo: LSTM vs GRU\n",
    "\n",
    "#### ¿Cuál arquitectura presenta mejor desempeño?\n",
    "\n",
    "Comparación promedio de métricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5015ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar promedios de LSTM vs GRU\n",
    "lstm_avg = lstm_comparison_df[['MSE', 'MAE', 'RMSE', 'R²', 'MAPE (%)']].mean()\n",
    "gru_avg = gru_comparison_df[['MSE', 'MAE', 'RMSE', 'R²', 'MAPE (%)']].mean()\n",
    "\n",
    "comparison_avg = pd.DataFrame({\n",
    "    'LSTM (promedio)': lstm_avg,\n",
    "    'GRU (promedio)': gru_avg,\n",
    "    'Diferencia': gru_avg - lstm_avg\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARACIÓN PROMEDIO: LSTM vs GRU\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_avg)\n",
    "\n",
    "# Determinar ganador\n",
    "if lstm_avg['MAPE (%)'] < gru_avg['MAPE (%)']:\n",
    "    winner = \"LSTM\"\n",
    "    diff = gru_avg['MAPE (%)'] - lstm_avg['MAPE (%)']\n",
    "else:\n",
    "    winner = \"GRU\"\n",
    "    diff = lstm_avg['MAPE (%)'] - gru_avg['MAPE (%)']\n",
    "\n",
    "print(f\"\\n Ganador: {winner} (MAPE {diff:.2f}% menor)\")\n",
    "\n",
    "# Análisis por configuración\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS POR CONFIGURACIÓN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "configs = ['Baseline', 'Seq=120', 'Seq=180', 'Batch=32', 'Batch=64', 'Dropout=0.2']\n",
    "for i, config in enumerate(configs):\n",
    "    lstm_mape = lstm_comparison_df.iloc[i]['MAPE (%)']\n",
    "    gru_mape = gru_comparison_df.iloc[i]['MAPE (%)']\n",
    "    better = \"LSTM\" if lstm_mape < gru_mape else \"GRU\"\n",
    "    diff = abs(lstm_mape - gru_mape)\n",
    "    print(f\"{config:20s}: LSTM={lstm_mape:6.2f}% | GRU={gru_mape:6.2f}% | Mejor: {better:4s} (Δ={diff:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314594aa",
   "metadata": {},
   "source": [
    "### 6.3 Análisis de Hiperparámetros\n",
    "\n",
    "#### Efecto de la longitud de secuencia (seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar efecto de seq_length\n",
    "seq_configs = ['Baseline', 'Seq=120', 'Seq=180']\n",
    "seq_lengths = [60, 120, 180]\n",
    "\n",
    "lstm_seq_mape = [lstm_comparison_df.iloc[i]['MAPE (%)'] for i in [0, 1, 2]]\n",
    "gru_seq_mape = [gru_comparison_df.iloc[i]['MAPE (%)'] for i in [0, 1, 2]]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(seq_lengths, lstm_seq_mape, marker='o', linewidth=2, markersize=8, label='LSTM')\n",
    "ax.plot(seq_lengths, gru_seq_mape, marker='s', linewidth=2, markersize=8, label='GRU')\n",
    "ax.set_xlabel('Longitud de Secuencia', fontsize=12)\n",
    "ax.set_ylabel('MAPE (%)', fontsize=12)\n",
    "ax.set_title('Efecto de la Longitud de Secuencia en el Error', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/seq_length_effect.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"EFECTO DE LONGITUD DE SECUENCIA:\")\n",
    "print(\"-\" * 60)\n",
    "for i, (config, seq) in enumerate(zip(seq_configs, seq_lengths)):\n",
    "    print(f\"Seq={seq:3d}: LSTM={lstm_seq_mape[i]:6.2f}% | GRU={gru_seq_mape[i]:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb98eb",
   "metadata": {},
   "source": [
    "#### Efecto del tamaño de batch (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar efecto de batch_size\n",
    "batch_configs = ['Baseline', 'Batch=32', 'Batch=64']\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "lstm_batch_mape = [lstm_comparison_df.iloc[i]['MAPE (%)'] for i in [0, 3, 4]]\n",
    "gru_batch_mape = [gru_comparison_df.iloc[i]['MAPE (%)'] for i in [0, 3, 4]]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(batch_sizes, lstm_batch_mape, marker='o', linewidth=2, markersize=8, label='LSTM')\n",
    "ax.plot(batch_sizes, gru_batch_mape, marker='s', linewidth=2, markersize=8, label='GRU')\n",
    "ax.set_xlabel('Tamaño de Batch', fontsize=12)\n",
    "ax.set_ylabel('MAPE (%)', fontsize=12)\n",
    "ax.set_title('Efecto del Tamaño de Batch en el Error', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/batch_size_effect.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"EFECTO DE TAMAÑO DE BATCH:\")\n",
    "print(\"-\" * 60)\n",
    "for i, (config, batch) in enumerate(zip(batch_configs, batch_sizes)):\n",
    "    print(f\"Batch={batch:2d}: LSTM={lstm_batch_mape[i]:6.2f}% | GRU={gru_batch_mape[i]:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a059a3a",
   "metadata": {},
   "source": [
    "### 6.4 Análisis de Modelos Apilados\n",
    "\n",
    "#### ¿Los modelos apilados mejoran el desempeño?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babac8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar modelos simples vs apilados\n",
    "simple_lstm = lstm_comparison_df.iloc[0]  # Baseline\n",
    "simple_gru = gru_comparison_df.iloc[0]    # Baseline\n",
    "\n",
    "stacked_lstm_2 = stacked_comparison_df.iloc[0]  # 2 layers\n",
    "stacked_lstm_3 = stacked_comparison_df.iloc[1]  # 3 layers\n",
    "stacked_gru_2 = stacked_comparison_df.iloc[2]   # 2 layers\n",
    "stacked_gru_3 = stacked_comparison_df.iloc[3]   # 3 layers\n",
    "\n",
    "comparison_stacked = pd.DataFrame({\n",
    "    'Modelo': [\n",
    "        'LSTM Simple',\n",
    "        'LSTM Stacked 2L',\n",
    "        'LSTM Stacked 3L',\n",
    "        'GRU Simple',\n",
    "        'GRU Stacked 2L',\n",
    "        'GRU Stacked 3L'\n",
    "    ],\n",
    "    'MAPE (%)': [\n",
    "        simple_lstm['MAPE (%)'],\n",
    "        stacked_lstm_2['MAPE (%)'],\n",
    "        stacked_lstm_3['MAPE (%)'],\n",
    "        simple_gru['MAPE (%)'],\n",
    "        stacked_gru_2['MAPE (%)'],\n",
    "        stacked_gru_3['MAPE (%)']\n",
    "    ],\n",
    "    'R²': [\n",
    "        simple_lstm['R²'],\n",
    "        stacked_lstm_2['R²'],\n",
    "        stacked_lstm_3['R²'],\n",
    "        simple_gru['R²'],\n",
    "        stacked_gru_2['R²'],\n",
    "        stacked_gru_3['R²']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARACIÓN: MODELOS SIMPLES vs APILADOS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_stacked.to_string(index=False))\n",
    "\n",
    "# Visualizar comparación\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "models = ['Simple', '2-Layers', '3-Layers']\n",
    "lstm_mapes = [simple_lstm['MAPE (%)'], stacked_lstm_2['MAPE (%)'], stacked_lstm_3['MAPE (%)']]\n",
    "gru_mapes = [simple_gru['MAPE (%)'], stacked_gru_2['MAPE (%)'], stacked_gru_3['MAPE (%)']]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, lstm_mapes, width, label='LSTM', alpha=0.8)\n",
    "ax1.bar(x + width/2, gru_mapes, width, label='GRU', alpha=0.8)\n",
    "ax1.set_xlabel('Arquitectura', fontsize=12)\n",
    "ax1.set_ylabel('MAPE (%)', fontsize=12)\n",
    "ax1.set_title('Comparación: Simple vs Apilados (MAPE)', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "lstm_r2s = [simple_lstm['R²'], stacked_lstm_2['R²'], stacked_lstm_3['R²']]\n",
    "gru_r2s = [simple_gru['R²'], stacked_gru_2['R²'], stacked_gru_3['R²']]\n",
    "\n",
    "ax2.bar(x - width/2, lstm_r2s, width, label='LSTM', alpha=0.8)\n",
    "ax2.bar(x + width/2, gru_r2s, width, label='GRU', alpha=0.8)\n",
    "ax2.set_xlabel('Arquitectura', fontsize=12)\n",
    "ax2.set_ylabel('R²', fontsize=12)\n",
    "ax2.set_title('Comparación: Simple vs Apilados (R²)', fontsize=13, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/simple_vs_stacked.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rn-python310-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
